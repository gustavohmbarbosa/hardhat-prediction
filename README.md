# Helmet YOLO Reducer

Este projeto tem como objetivo **treinar** e, em seguida, **reduzir** o tamanho de um modelo YOLO para detecÃ§Ã£o de **capacete** (1 classe).

## âš™ï¸ Metodologia

### 0) Preparar o dataset | `prepare_helmet_dataset.py`

```bash
python prepare_helmet_dataset.py
```

* Faz o download do dataset do Kaggle (se jÃ¡ nÃ£o existir em `dataset/`).
* Aplica a filtragem dos rÃ³tulos, mantendo **apenas a classe 0 (capacete)**.
* Gera uma estrutura de pastas no formato YOLO:

  ```
  dataset/
    images/{train,val,test}
    labels/{train,val,test}
  ```

>âš ï¸ **Nota**: Ã© necessÃ¡rio ter as credenciais do Kaggle configuradas (`kaggle.json`). Saiba mais em https://www.kaggle.com/docs/api#authentication.

### 1) Treino baseline | `train_baseline.py`

```bash
python train_baseline.py
```

* Treina o **YOLOv8n (nano)** prÃ©-treinado no COCO.
* Usa hiperparÃ¢metros fixos definidos no cÃ³digo (ex.: `imgsz=320`, `epochs=60`, `batch=16`).
* Serve como **referÃªncia** para comparaÃ§Ã£o com versÃµes reduzidas.

### 2) Treino reduzido | `train_reduced.py`

```bash
python train_reduced.py
```

* Treina uma arquitetura **modificada** a partir de um `.yaml` em `models/` (ex.: `yolov8n_min.yaml`).
* O `.yaml` usa multiplicadores menores de **depth** e **width**, reduzindo:

  * **ParÃ¢metros (M)**
  * **FLOPs (G)**
  * **Tamanho do arquivo**
* Objetivo: encontrar um ponto de equilÃ­brio entre **tamanho menor** e **mAP aceitÃ¡vel**.

### 3) Poda do modelo (descartado) | `prune_and_finetune.py`

> Objetivo: **reduzir ainda mais** o tamanho/complexidade do modelo **depois** do treino, removendo canais pouco importantes (**structured channel pruning**) e, opcionalmente, fazer um **fine-tune** curto para recuperar mAP.

#### O que o script faz

1. **Carrega** um checkpoint treinado (`best.pt`) do Ultralytics YOLO.
2. **Calcula importÃ¢ncia** dos canais (L1/Magnitude) e seleciona **apenas** `Conv2d` com `groups==1` (evita depthwise/DFL/Detect).
3. **Tenta podar estruturalmente** (remover canais do grafo, nÃ£o sÃ³ zerar pesos).
4. **Salva** o resultado
5. (Opcional) **Fine-tune** rÃ¡pido no modelo podado para **recuperar a acurÃ¡cia**.

> Nota: em arquiteturas com **atalhos/concats** (como YOLOv8 com blocos C2f e mÃºltiplos heads), a poda estrutural Ã© **mais restrita**. Se o grafo nÃ£o puder ser ajustado com seguranÃ§a, o script evita cortes perigosos (isso pode resultar em â€œsem mudanÃ§a de Params/FLOPsâ€). A alternativa mais robusta e simples Ã© **reduzir a arquitetura pelo YAML** (width/depth), e usar **quantizaÃ§Ã£o** para diminuir arquivo/latÃªncia.

#### Uso

```bash
# dependÃªncias
pip install torch-pruning ultralytics thop onnx onnxruntime

# execuÃ§Ã£o
python prune_and_finetune.py \
  --weights runs_train/<seu_run>/weights/best.pt \
  --data hardhat.yaml \
  --imgsz 320 \
  --epochs 30 \        # 0 = nÃ£o faz fine-tune
  --batch 16 \
  --sparsity 0.35 \    # 0.2~0.35 = moderado; >0.5 = agressivo
  --device cuda:0
```

##### Argumentos principais

* `--weights`: checkpoint treinado a ser podado (ex.: `best.pt`).
* `--data`: YAML do dataset (ex.: `hardhat.yaml`) â€” usado no fine-tune.
* `--imgsz`: tamanho de entrada para traÃ§ar o grafo.
* `--epochs`: Ã©pocas de **fine-tune pÃ³s-poda** (0 para pular).
* `--batch`: batch size do fine-tune.
* `--sparsity`: **proporÃ§Ã£o global de canais a remover** (0.0â€“0.9).
* `--device`: `cuda:0` para GPU, ou `cpu`.

##### SaÃ­das esperadas

* `pruned_full.pt` â†’ **nn.Module** podado (estrutura menor).
* `pruned.onnx` â†’ export estruturado (se `onnx` instalado).
* Novo run em `runs_prune/<exp>/` se `--epochs > 0` (fine-tune), com `weights/best.pt` e mÃ©tricas.

#### Como avaliar

* Rode `python summarize_runs.py` para comparar **Params, FLOPs, Size, Latency e mAP** antes Ã— depois.
* Se **Params/FLOPs nÃ£o mudarem**, a poda **nÃ£o foi estruturalmente aplicada** naquela topologia


### 4) Resumir resultados | `summarize_runs.py`

```bash
python summarize_runs.py
```

* Coleta automaticamente os resultados em `runs_train/*` e `runs_prune/*`.
* Gera um relatÃ³rio `.csv` e `.md` com mÃ©tricas dos modelos.

Exemplo:
| Kind   | Run                           | Stage        | Model            |   ImgSize |   Epochs |   Batch | File                                          |   Size (MB) |   mAP@0.5 |   mAP@0.5:0.95 |   Train Time (min) |   Params (M) |   FLOPs (G) |   Latency (ms) |
|:-------|:------------------------------|:-------------|:-----------------|----------:|---------:|--------:|:----------------------------------------------|------------:|----------:|---------------:|-------------------:|-------------:|------------:|---------------:|
| run    | runs_train\reduced_min_320_v2 | reduced_arch | yolov8n_min.yaml |       320 |      100 |      32 | runs_train\reduced_min_320_v2\weights\best.pt |       3.109 |    0.861  |         0.5402 |              211.4 |       1.5387 |      0.5748 |          6.21  |
| run    | runs_train\reduced_min_320    | reduced_arch | yolov8n_min.yaml |       320 |       10 |      16 | runs_train\reduced_min_320\weights\best.pt    |       3.603 |    0.7638 |         0.4493 |               18.7 |       1.7992 |      0.6469 |          6.569 |
| run    | runs_train\baseline_v8n_320   |              | yolov8n.pt       |       320 |       60 |      16 | runs_train\baseline_v8n_320\weights\best.pt   |       5.923 |    0.8879 |         0.5803 |               89.7 |       3.011  |      1.0243 |          6.965 |

## ExplicaÃ§Ã£o das colunas

* **Kind** â†’ tipo de entrada (`run` = treino, `pruned_ckpt` = modelo podado sem fine-tune).
* **Run** â†’ diretÃ³rio do experimento.
* **Stage** â†’ estÃ¡gio do processo (baseline, reduced\_arch, prune, etc).
* **Model** â†’ qual YAML ou checkpoint foi usado.
* **ImgSize** â†’ resoluÃ§Ã£o de entrada (pixels).
* **Epochs** â†’ nÃºmero de Ã©pocas de treino.
* **Batch** â†’ tamanho do batch.
* **File** â†’ caminho para o `best.pt`.
* **Size (MB)** â†’ tamanho do arquivo em disco.
* **mAP\@0.5** â†’ mÃ©dia da precisÃ£o mÃ©dia com IoU=0.5 (permissivo).
* **mAP\@0.5:0.95** â†’ mÃ©trica oficial COCO (IoU=0.5:0.95, mais rigorosa).
* **Train Time (min)** â†’ duraÃ§Ã£o estimada do treino.
* **Params (M)** â†’ nÃºmero de parÃ¢metros (em milhÃµes).
* **FLOPs (G)** â†’ custo computacional (em bilhÃµes de operaÃ§Ãµes).
* **Latency (ms)** â†’ tempo mÃ©dio de inferÃªncia de 1 imagem (ms).

## ğŸ”— Ãštil

* [DescriÃ§Ã£o de parÃ¢metros YOLO](./YOLO.md)
* [Script de verificaÃ§Ã£o GPU/CUDA](./verify-cuda.py):

  ```bash
  python verify-cuda.py
  ```

## Outros Scripts

### ğŸ“¸ PrediÃ§Ã£o em uma imagem | `predict_one.py`

**O que faz:**
Carrega o modelo escolhido, executa a **prediÃ§Ã£o em uma Ãºnica imagem**, salva:

* a **imagem com as detecÃ§Ãµes desenhadas** (`outdir/vis/<nome>_pred.jpg`);
* um **.txt** com as detecÃ§Ãµes (classe, confianÃ§a, x1 y1 x2 y2).

**Uso:**

```bash
python predict_one.py \
  --weights runs_train/reduced_min_320_v2/weights/best.pt \
  --image dataset/images/test/obra_101.jpg \
  --outdir predict_out \
  --imgsz 320 \
  --conf 0.25
```

**ParÃ¢metros principais:**

* `--weights` â†’ caminho do modelo (.pt).
* `--image` â†’ caminho da imagem de entrada.
* `--outdir` â†’ pasta de saÃ­da (default: `predict_out`).
* `--imgsz` â†’ tamanho de entrada para a inferÃªncia (default: 320).
* `--conf` â†’ limiar de confianÃ§a (default: 0.25).

**SaÃ­das:**

* `predict_out/vis/obra_101_pred.jpg` (overlay com as caixas)
* `predict_out/obra_101_pred.txt` (linhas: `class conf x1 y1 x2 y2`)

---

### ğŸ§ª Teste no conjunto de teste (ou subset) | `test_model.py`

Compara **Pred vs Real** em `images/test`.

**O que faz:**

* LÃª `images/test` e `labels/test` a partir do `hardhat.yaml`.
* Roda prediÃ§Ã£o, faz **matching por IoU** (padrÃ£o `0.5`) e contabiliza **TP/FP/FN**.
* Gera **visualizaÃ§Ãµes** com **GT (verde)** vs **Pred (vermelho)**.
* Salva **CSV por imagem** e **summary.json** com mÃ©tricas agregadas.

**Casos de uso:**

1. **Avaliar todo o test:**

```bash
python test_model.py \
  --weights runs_train/reduced_min_320_v2/weights/best.pt \
  --data hardhat.yaml \
  --outdir eval_out \
  --imgsz 320 \
  --conf 0.25 \
  --nms_iou 0.5 \
  --match_iou 0.5
```

2. **Avaliar somente 3 imagens especÃ­ficas (pelos nomes-base):**

```bash
python test_model.py \
  --weights runs_train/reduced_min_320_v2/weights/best.pt \
  --data hardhat.yaml \
  --outdir eval_out_subset \
  --imgsz 320 \
  --conf 0.25 \
  --nms_iou 0.5 \
  --match_iou 0.5 \
  --subset obra_101,obra_202,capacete_777
```

**ParÃ¢metros principais:**

* `--weights` â†’ modelo (.pt) a avaliar.
* `--data` â†’ YAML do dataset (ex.: `hardhat.yaml`).
* `--outdir` â†’ pasta de saÃ­da (default: `eval_out`).
* `--imgsz` â†’ tamanho de entrada da inferÃªncia (default: 320).
* `--conf` â†’ limiar de confianÃ§a (default: 0.25).
* `--nms_iou` â†’ IoU para NMS das prediÃ§Ãµes (default: 0.5).
* `--match_iou` â†’ IoU para considerar **TP** no matching PredÃ—GT (default: 0.5).
* `--subset` â†’ lista de nomes-base (sem extensÃ£o) separados por vÃ­rgula para avaliar sÃ³ um subconjunto.

**SaÃ­das:**

* `eval_out/vis/<nome>_cmp.jpg` â†’ imagem com **GT (verde)** e **Pred (vermelho)**.
* `eval_out/per_image.csv` â†’ mÃ©tricas por imagem (`TP/FP/FN`, `precision`, `recall`, `f1`, `mean_IoU@match_iou`).
* `eval_out/summary.json` â†’ agregado global (totais de TP/FP/FN e mÃ©tricas globais).

**Leitura rÃ¡pida das mÃ©tricas:**

* **precision** alto / **recall** baixo â†’ modelo â€œexigenteâ€ (menos falsos positivos, mais falsos negativos).
* **recall** alto / **precision** baixo â†’ modelo â€œpermissivoâ€ (mais cobertura, porÃ©m mais falsos positivos).
* **mean\_IoU\@0.5** â†’ quÃ£o bem as caixas **corretas** sobrepÃµem o GT (mÃ©dia de IoU dos TPs).
